{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Implementing a VQVAE in JAX for fun 😎\"\n",
    "author:\n",
    "  - name: \"Tugdual Kerjan\"\n",
    "    url: https://tugdual.fr\n",
    "    email: tkerjan@outlook.com\n",
    "date: \"November 6, 2024\"\n",
    "number-sections: true\n",
    "reference-location: margin\n",
    "toc: true\n",
    "format: \n",
    "  html:\n",
    "    standalone: true\n",
    "    embed-resources: true\n",
    "    self-contained-math: true\n",
    "    code-fold: false\n",
    "    code-tools: true\n",
    "execute:\n",
    "  output:\n",
    "    false\n",
    "bibliography: bibtext.bibtex\n",
    "filters:\n",
    "  - code-visibility\n",
    "theme: united\n",
    "github: \"https://github.com/TugdualKerjan/Audio-VQVAE-for-JAX\"\n",
    "lightbox: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full project, visit the [GitHub repository](https://github.com/TugdualKerjan/Audio-VQVAE-for-JAX).\n",
    "\n",
    "# Context 👀\n",
    "\n",
    "I'm trying to rewrite XTTS in JAX to understand how it works. \n",
    "\n",
    "We are going to implement the VQVAE used in [@casanova2024xttsmassivelymultilingualzeroshot], a Text to Speech model written by the defunct Coqai company. VQVAEs come from [@oord2018neuraldiscreterepresentationlearning]\n",
    "\n",
    "VQVAE means Vector Quantized Variational AutoEncoder. Let's break down the name. \n",
    "\n",
    "__Auto-Encoder__\n",
    "\n",
    "This model takes the input, passes it through smaller layers (Encodes) and tries to, from that small layer, reproduce the input (Decodes). It thus needs to learn what are the most important features to keep ! If the input was images of cats, the model would keep information about the color, way it's looking, thiccness of the cat.\n",
    "\n",
    "__Variational__\n",
    "\n",
    "Instead of having to encode inputs onto points in space we map them onto distributions. In our case, we simply want to make sure that on average, the space we're mapping to is uniform. This means all _codes_ will be used.\n",
    "\n",
    "__Vector Quantized__\n",
    "\n",
    "We want to define a set of points in the smaller representation, the latent. These points will be called _codes_ and be part of a _codebook_. Think of the codebook as the set of possible words the encoder can use to describe what it sees to the decoder. Obviously, the more codes we have, the more information the encoder will be able to pass per code sent.\n",
    "\n",
    "![Overview of the model [@csdn_image_2024]](assets/vqvae.png)\n",
    "\n",
    "# Goal 🎯\n",
    "\n",
    "Our VQVAE is going to find the best codes that describe speech. It'll take in special images called Mel-Frequency Spectrograms, which is basically a way to represent human speech.\n",
    "\n",
    "Since our final goal is to recreate a 1 to 1 version of the VQVAE used in XTTS, we'll hardcode a lot of things to minimize issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "DiscreteVAE(\n",
    "  (discrete_loss): DiscretizationLoss()\n",
    "  (encoder): Sequential(\n",
    "    (0): Sequential(\n",
    "      (0): Conv1d(80, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
    "      (1): ReLU()\n",
    "    )\n",
    "    (1): Sequential(\n",
    "      (0): Conv1d(512, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
    "      (1): ReLU()\n",
    "    )\n",
    "    (2): ResBlock(\n",
    "      (net): Sequential(\n",
    "        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "        (1): ReLU()\n",
    "        (2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "        (3): ReLU()\n",
    "        (4): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
    "      )\n",
    "    )\n",
    "    (3): ResBlock(\n",
    "      (net): Sequential(\n",
    "        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "        (1): ReLU()\n",
    "        (2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "        (3): ReLU()\n",
    "        (4): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
    "      )\n",
    "    )\n",
    "    (4): ResBlock(\n",
    "      (net): Sequential(\n",
    "        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "        (1): ReLU()\n",
    "        (2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "        (3): ReLU()\n",
    "        (4): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
    "      )\n",
    "    )\n",
    "    (5): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
    "  )\n",
    "  (decoder): Sequential(\n",
    "    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
    "    (1): ResBlock(\n",
    "      (net): Sequential(\n",
    "        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "        (1): ReLU()\n",
    "        (2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "        (3): ReLU()\n",
    "        (4): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
    "      )\n",
    "    )\n",
    "    (2): ResBlock(\n",
    "      (net): Sequential(\n",
    "        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "        (1): ReLU()\n",
    "        (2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "        (3): ReLU()\n",
    "        (4): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
    "      )\n",
    "    )\n",
    "    (3): ResBlock(\n",
    "      (net): Sequential(\n",
    "        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "        (1): ReLU()\n",
    "        (2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "        (3): ReLU()\n",
    "        (4): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
    "      )\n",
    "    )\n",
    "    (4): Sequential(\n",
    "      (0): UpsampledConv(\n",
    "        (conv): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "      )\n",
    "      (1): ReLU()\n",
    "    )\n",
    "    (5): Sequential(\n",
    "      (0): UpsampledConv(\n",
    "        (conv): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
    "      )\n",
    "      (1): ReLU()\n",
    "    )\n",
    "    (6): Conv1d(512, 80, kernel_size=(1,), stride=(1,))\n",
    "  )\n",
    "  (codebook): Quantize()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We have 4 parts to code: An ResBlock, Encoder, a Decoder and a Quantizer. Let's get into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing JAX and Equinox, a library that helps with writing neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "import typing as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResBlocks\n",
    "\n",
    "The role of this Block is to mainly exchange information between the various parts of each channel, but the input is added at the end. This allows our network to basically, if suitable, simply put all weights to zero and be \"Shallower\" basically our network decides how many layers it needs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class ResBlock(eqx.Module):\n",
    "    conv1: nn.Conv1d\n",
    "    conv2: nn.Conv1d\n",
    "    conv3: nn.Conv1d\n",
    "    act: tp.Callable = eqx.static_field()\n",
    "\n",
    "    def __init__(self, dim: int, activation=jax.nn.relu, key=None):\n",
    "\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(dim, dim, kernel_size=3, padding=\"SAME\", key=key1)\n",
    "        self.conv2 = nn.Conv1d(dim, dim, kernel_size=3, padding=\"SAME\", key=key2)\n",
    "        self.conv3 = nn.Conv1d(dim, dim, kernel_size=1, padding=\"SAME\", key=key3)\n",
    "\n",
    "        self.act = activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = x\n",
    "\n",
    "        y = self.conv1(y)\n",
    "        y = jax.nn.relu(y)\n",
    "        y = self.conv2(y)\n",
    "        y = jax.nn.relu(y)\n",
    "        y = self.conv3(y)\n",
    "\n",
    "        y = y + x\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Moving onto the Encoder. It has layers that take in the input, and slowly compress it by lowering the image dimensions and increasing the amounts of channels, much like ResNet:\n",
    "\n",
    "::: {#fig-encode}\n",
    "\n",
    "![Visualisation of what the encoder is doing [@dumakude2023automated_image]](assets/encode.png)\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(eqx.Module):\n",
    "    conv1: nn.Conv1d\n",
    "    conv2: nn.Conv1d\n",
    "    conv3: nn.Conv1d\n",
    "    res1: ResBlock\n",
    "    res2: ResBlock\n",
    "    res3: ResBlock\n",
    "\n",
    "    def __init__(self, hidden_dim: int = 1024, codebook_dim: int = 512, key=None):\n",
    "        key1, key2, key3, key4, key5, key6 = jax.random.split(key, 6)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=80,\n",
    "            out_channels=512,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=\"SAME\",\n",
    "            key=key1,\n",
    "        )\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=512,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=\"SAME\",\n",
    "            key=key2,\n",
    "        )\n",
    "        self.res1 = ResBlock(dim=hidden_dim, key=key3)\n",
    "        self.res2 = ResBlock(dim=hidden_dim, key=key4)\n",
    "        self.res3 = ResBlock(dim=hidden_dim, key=key5)\n",
    "        self.conv3 = nn.Conv1d(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=codebook_dim,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=\"SAME\",\n",
    "            key=key6,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        y = self.conv1(x)\n",
    "        y = jax.nn.relu(y)\n",
    "        y = self.conv2(y)\n",
    "        y = jax.nn.relu(y)\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.conv3(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "We can now implement the decoder. Instead of using ConvTranspose1d here XTTS uses upsampling and interpolation between points, replacing the striding that would usually happen. We implement it just below @sec-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class Decoder(eqx.Module):\n",
    "    conv1: nn.Conv1d\n",
    "    conv2: UpsampledConv\n",
    "    conv3: UpsampledConv\n",
    "    conv4: nn.Conv1d\n",
    "    res1: ResBlock\n",
    "    res2: ResBlock\n",
    "    res3: ResBlock\n",
    "\n",
    "    def __init__(self, hidden_dim: int = 1024, codebook_dim: int = 512, key=None):\n",
    "        key1, key2, key3, key4, key5, key6, key7 = jax.random.split(key, 7)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=codebook_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=\"SAME\",\n",
    "            key=key1,\n",
    "        )\n",
    "        self.res1 = ResBlock(dim=hidden_dim, key=key2)\n",
    "        self.res2 = ResBlock(dim=hidden_dim, key=key3)\n",
    "        self.res3 = ResBlock(dim=hidden_dim, key=key4)\n",
    "        self.conv2 = UpsampledConv(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=\"SAME\",\n",
    "            key=key5,\n",
    "        )\n",
    "        self.conv3 = UpsampledConv(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=512,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=\"SAME\",\n",
    "            key=key6,\n",
    "        )\n",
    "        self.conv4 = nn.Conv1d(\n",
    "            in_channels=512,\n",
    "            out_channels=80,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=\"SAME\",\n",
    "            key=key7,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        y = self.conv1(x)\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.conv2(y)\n",
    "        y = jax.nn.relu(y)\n",
    "        y = self.conv3(y)\n",
    "        y = jax.nn.relu(y)\n",
    "        y = self.conv4(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UpsampledConv {#sec-plot}\n",
    "\n",
    "Before we move onto the decoder we have to define a special layer that replaces the ConvTranspose that we would normally use. I admit I still am not sure of the use of this.\n",
    "\n",
    "Their code for this function:\n",
    "\n",
    "```python\n",
    "class UpsampledConv(nn.Module):\n",
    "    def __init__(self, conv, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        assert \"stride\" in kwargs.keys()\n",
    "        self.stride = kwargs[\"stride\"]\n",
    "        del kwargs[\"stride\"]\n",
    "        self.conv = conv(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        up = nn.functional.interpolate(x, scale_factor=self.stride, mode=\"nearest\")\n",
    "        return self.conv(up)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute the torch version to check how it works and then compare to our solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "\n",
    "class UpsampledConv(torch.nn.Module):\n",
    "    def __init__(self, conv, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        assert \"stride\" in kwargs.keys()\n",
    "        self.stride = kwargs[\"stride\"]\n",
    "        del kwargs[\"stride\"]\n",
    "        self.conv = conv(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        up = torch.nn.functional.interpolate(\n",
    "            x, scale_factor=self.stride, mode=\"nearest\"\n",
    "        )\n",
    "        print(up.shape)\n",
    "        return self.conv(up)\n",
    "\n",
    "\n",
    "upsamp = UpsampledConv(torch.nn.Conv1d, 80, 512, 3, stride=2, padding=1)\n",
    "x = torch.ones((3, 80, 100))\n",
    "print(upsamp(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write our own version using JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# | label: upsample\n",
    "\n",
    "\n",
    "class UpsampledConv(eqx.Module):\n",
    "    conv: nn.Conv1d\n",
    "    stride: int = eqx.static_field()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: tp.Union[int, tp.Tuple[int]],\n",
    "        stride: int,\n",
    "        padding: tp.Union[int, str],\n",
    "        key=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padding,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        upsampled_size = (x.shape[0], x.shape[1] * self.stride)\n",
    "        upsampled = jax.image.resize(x, upsampled_size, method=\"nearest\")\n",
    "        return self.conv(upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code below allows to do a quick check to see it works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "key = jax.random.PRNGKey(1)\n",
    "upsamp = UpsampledConv(80, 150, kernel_size=3, stride=2, padding=\"SAME\", key=key)\n",
    "x = jax.random.normal(key, shape=(80, 100))\n",
    "y = upsamp(x)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having implemented various VQVAEs, what strikes me in this one is that the ResBlocks are all seperated from the various convolutional stages, and the lack of normalisation between layers. Moving onto the crux of the matter, the Quantizer !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantizer\n",
    "\n",
    "So our encoder spits out a certain number of vectors (The number of channels) with the codebook dim. These vectors are mapped to their nearest neighbors, and these are then transmitted to the decoder. To stabalise the model, we're going to also add some exponential moving average to the codebook, as well as normalize things so as to keep things from exploding or minimizing. Exponential moving average meaning that we basically mostly keep what we currently have and add a little of the new stuff instead of fully changing things every time.\n",
    "\n",
    "$${Codebook} = {Codebook}_{old} * decay + {Codebook}_{new} * (1 - decay)$$\n",
    "\n",
    "We can't update the codebook here though, as this is immutable stuff. We need to update it between each training instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# | output: true\n",
    "\n",
    "\n",
    "class Quantizer(eqx.Module):\n",
    "    K: int = eqx.static_field()\n",
    "    D: int = eqx.static_field()\n",
    "    codebook: jax.Array\n",
    "\n",
    "    codebook_avg: jax.Array\n",
    "    cluster_size: jax.Array\n",
    "\n",
    "    decay: float = eqx.static_field()\n",
    "    eps: float = eqx.static_field()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_vecs: int = 1024,\n",
    "        num_dims: int = 512,\n",
    "        decay: float = 0.99,\n",
    "        eps: float = 1e-5,\n",
    "        key=None,\n",
    "    ):\n",
    "        self.K = num_vecs\n",
    "        self.D = num_dims\n",
    "\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "\n",
    "        # Init a matrix of vectors that will move with time\n",
    "        self.codebook = jax.nn.initializers.variance_scaling(\n",
    "            scale=1.0, mode=\"fan_in\", distribution=\"uniform\"\n",
    "        )(key, (num_vecs, num_dims))\n",
    "        self.codebook_avg = jnp.copy(self.codebook)\n",
    "        self.cluster_size = jnp.zeros(num_vecs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x has N vectors of the codebook dimension. We calculate the nearest neighbors and output those instead\n",
    "\n",
    "        flatten = jax.numpy.reshape(x, (-1, self.D))\n",
    "        a_squared = jnp.sum(flatten**2, axis=-1, keepdims=True)\n",
    "        b_squared = jnp.transpose(jnp.sum(self.codebook**2, axis=-1, keepdims=True))\n",
    "        distance = (\n",
    "            a_squared\n",
    "            + b_squared\n",
    "            - 2 * jnp.matmul(flatten, jnp.transpose(self.codebook))\n",
    "        )\n",
    "\n",
    "        codebook_indices = jnp.argmin(distance, axis=-1)\n",
    "\n",
    "        z_q = self.codebook[codebook_indices]\n",
    "\n",
    "        # Straight-through estimator\n",
    "        z_q = flatten + jax.lax.stop_gradient(z_q - flatten)\n",
    "\n",
    "        z_q = jax.numpy.permute_dims(z_q, (1, 0))\n",
    "\n",
    "        return z_q, self.codebook_updates(flatten, codebook_indices)\n",
    "\n",
    "    def codebook_updates(self, flatten, codebook_indices):\n",
    "\n",
    "        # Calculate the usage of various codes.\n",
    "        codebook_onehot = jax.nn.one_hot(codebook_indices, self.K)\n",
    "        codebook_onehot_sum = jnp.sum(codebook_onehot, axis=0)\n",
    "        codebook_sum = jnp.dot(flatten.T, codebook_onehot)\n",
    "        # We've just weighed the codebook vectors.\n",
    "\n",
    "        # Basically count on average how many codes we're using\n",
    "        new_cluster_size = (\n",
    "            self.decay * self.cluster_size + (1 - self.decay) * codebook_onehot_sum\n",
    "        )\n",
    "\n",
    "        # Where is the average embedding at ?\n",
    "        new_codebook_avg = (\n",
    "            self.decay * self.codebook_avg + (1 - self.decay) * codebook_sum.T\n",
    "        )\n",
    "\n",
    "        n = jnp.sum(new_cluster_size)  # Over the total embeddings used\n",
    "        new_cluster_size = (new_cluster_size + self.eps) / (n + self.K * self.eps) * n\n",
    "        new_codebook = self.codebook_avg / new_cluster_size[:, None]\n",
    "\n",
    "        updates = (new_cluster_size, new_codebook_avg, new_codebook)\n",
    "\n",
    "        return updates, codebook_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize vectors being mapped to the neasest codes below, where light blue vectors come in, and are snapped to the closest red vectors. TODO fix the bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | label: quantized\n",
    "# | output: true\n",
    "# | fig-cap: Vectors finding the nearest code\n",
    "# | column: margin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "key = jax.random.PRNGKey(3)\n",
    "\n",
    "key1, key2 = jax.random.split(key)\n",
    "\n",
    "quantizer = Quantizer(7, 2, key=key1)\n",
    "x = jax.random.normal(key2, shape=(2, 2, 3)) / 2\n",
    "\n",
    "y, _ = jax.vmap(quantizer)(x)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 8))\n",
    "axs = axs.flatten()  # Flatten for easy iteration\n",
    "\n",
    "# Plot data on each subplot (adjust as needed)\n",
    "for i, ax in enumerate(axs):\n",
    "    origin_x = jnp.zeros(3)\n",
    "    origin_y = jnp.zeros(3)\n",
    "\n",
    "    # Plot arrows with quiver\n",
    "    ax.quiver(\n",
    "        origin_x,\n",
    "        origin_y,\n",
    "        y[i, 1, :],\n",
    "        y[i, 0, :],\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=1,\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    ax.quiver(\n",
    "        origin_x,\n",
    "        origin_y,\n",
    "        x[i, 1, :],\n",
    "        x[i, 0, :],\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=1,\n",
    "        color=\"blue\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    ax.quiver(\n",
    "        jnp.zeros(quantizer.codebook.shape[0]),\n",
    "        jnp.zeros(quantizer.codebook.shape[0]),\n",
    "        quantizer.codebook[:, 1],\n",
    "        quantizer.codebook[:, 0],\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=1,\n",
    "        color=\"none\",\n",
    "        edgecolor=\"red\",\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "    ax.set_xlim(-0.7, 0.7)  # Adjust limits as needed\n",
    "    ax.set_ylim(-0.7, 0.7)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(f\"Example {i+1}\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "# Show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the above does is create K vectors in D dimensional space. Incoming vectors find their nearest match and the loss calculated is the L2 distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQVAE\n",
    "\n",
    "Let's finish up by defining a final Module that wraps all of this up, and defining our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class VQVAE(eqx.Module):\n",
    "    encoder: Encoder\n",
    "    decoder: Decoder\n",
    "    quantizer: Quantizer\n",
    "\n",
    "    def __init__(self, key=None):\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "\n",
    "        self.encoder = Encoder(key=key1)\n",
    "        self.decoder = Decoder(key=key2)\n",
    "        self.quantizer = Quantizer(decay=0.8, key=key3)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, codebook_indices = self.quantizer(z_e)\n",
    "        y = self.decoder(z_q)\n",
    "\n",
    "        return z_e, z_q, codebook_indices, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick test on the full model to make sure that the whole works correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: true\n",
    "\n",
    "import jax\n",
    "\n",
    "key1, key2 = jax.random.split(jax.random.PRNGKey(69))\n",
    "\n",
    "model = VQVAE(key1)\n",
    "x = jax.random.normal(key2, shape=(10, 80, 400))\n",
    "z_e, z_q, _, y = jax.vmap(model)(x)\n",
    "print(x.shape)\n",
    "print(z_e.shape)\n",
    "print(z_q.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can see that we go from a $[80 \\times X]$ long spectrogram and go down to a $[512 \\times \\frac{X}{4}]$ image, which to be fair seems kind of dumb because we actually don't loose any information like this. We could have a 1 to 1 reproduction of the image... was it not for the quantizer in the middle that forces us to assign the image to actually, only $\\frac{X}{4}$ vectors ! Whatever the dimension of these vectors, the information we encode can this be counted in $log_2(x/4)$ bits which makes it quite small 🤏😎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {#fig-table .column-margin}\n",
    "\n",
    "![](assets/chaos.png)\n",
    "\n",
    "How not to train things\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codebook special thingies\n",
    "\n",
    "There are a few things that we need to do. First we need to write a function that will update our model based on the passed ${Codebook}$. It should not only update the various values we're keeping track of, but since we're using a VQVAE and that they impose that the distribution is __uniform__ this means that we need to yeet codes that are being used too often. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "VAEs work because we impose that they don't just encode important information but they encode it in a uniform way, so that vectors are well spread out and not clustered. Normally you need to add a loss term checking at what point the embeddings follow that spread (usually a guassian distribution). In VQ-VAEs we impose the distribution to be uniform, making this term constant and thus we don't have to add it in the loss\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {layout-ncol=2}\n",
    "\n",
    "![With replacing codebook outliers](assets/spread.png){#fig-surus}\n",
    "\n",
    "![Without replacement](assets/close.png){#fig-hanno}\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "def update_codebook_ema(model: VQVAE, updates: tuple, codebook_indices, key=None):\n",
    "    avg_updates = jax.tree.map(lambda x: jax.numpy.mean(x, axis=0), updates)\n",
    "\n",
    "    # Calculate which codes are too often used and yeet them. Prior is uniform.\n",
    "    h = jnp.histogram(\n",
    "        codebook_indices, bins=model.quantizer.K, range=(0, model.quantizer.K)\n",
    "    )[0] / len(codebook_indices)\n",
    "    part_that_should_be = 1 / model.quantizer.K\n",
    "    mask = (h > 2 * part_that_should_be) | (h < 0.5 * part_that_should_be)\n",
    "    rand_embed = (\n",
    "        jax.random.normal(key, (model.quantizer.K, model.quantizer.D)) * mask[:, None]\n",
    "    )\n",
    "    avg_updates = (\n",
    "        avg_updates[0],\n",
    "        avg_updates[1],\n",
    "        jnp.where(mask[:, None], rand_embed, avg_updates[2]),\n",
    "    )\n",
    "\n",
    "    where = lambda q: (\n",
    "        q.quantizer.cluster_size,\n",
    "        q.quantizer.codebook_avg,\n",
    "        q.quantizer.codebook,\n",
    "    )\n",
    "\n",
    "    # Update the codebook and other trackers.\n",
    "    model = eqx.tree_at(where, model, avg_updates)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses and gradients\n",
    "\n",
    "Nearly there ! 😮‍💨 We can now write out our two \"classic\" functions, that will 1. Calculate at what point our boi outputs garbage or not, by comparing outputs to inputs using Mean Square Error, and at what point the encoder is outputing vectors that are close to the codes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "@eqx.filter_value_and_grad(has_aux=True)\n",
    "def calculate_losses(model, x):\n",
    "    z_e, z_q, codebook_updates, y = jax.vmap(model)(x)\n",
    "\n",
    "    # Are the inputs and outputs close?\n",
    "    reconstruct_loss = jnp.mean(jnp.linalg.norm((x - y), ord=2, axis=(1, 2)))\n",
    "\n",
    "    # Are the output vectors z_e close to the codes z_q ?\n",
    "    commit_loss = jnp.mean(\n",
    "        jnp.linalg.norm(z_e - jax.lax.stop_gradient(z_q), ord=2, axis=(1, 2))\n",
    "    )\n",
    "    # codebook = jnp.mean(codebook_updates[0][2], axis=0) #| hide_line\n",
    "    # print(codebook.shape) #| hide_line\n",
    "    # print(codebook) #| hide_line\n",
    "    # print(jnp.mean(codebook, axis=-1).shape) #| hide_line\n",
    "    # print(f\"STDR: {jnp.std(codebook, axis=-1)}\") #| hide_line\n",
    "    # print(f\"log: {jnp.log(jnp.clip(jnp.mean(codebook, axis=-1), min=1e-5)) }\") #| hide_line\n",
    "    # KL_loss = 0.5 * jnp.sum(jnp.mean(codebook, axis=-1)**2 + jnp.var(codebook, axis=-1)  #| hide_line- jnp.log(jnp.clip(jnp.std(codebook, axis=-1), min=1e-6)) - 1) #| hide_line\n",
    "\n",
    "    total_loss = reconstruct_loss + commit_loss\n",
    "\n",
    "    return total_loss, (reconstruct_loss, commit_loss, codebook_updates, y)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, optimizer, opt_state, x, key):\n",
    "    (total_loss, (reconstruct_loss, commit_loss, codebook_updates, y)), grads = (\n",
    "        calculate_losses(model, x)\n",
    "    )\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    model = update_codebook_ema(model, codebook_updates[0], codebook_updates[1], key)\n",
    "\n",
    "    return (\n",
    "        model,\n",
    "        opt_state,\n",
    "        total_loss,\n",
    "        reconstruct_loss,\n",
    "        commit_loss,\n",
    "        codebook_updates,\n",
    "        y,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "key = jax.random.PRNGKey(69)\n",
    "\n",
    "model = VQVAE(key=key)\n",
    "\n",
    "optimizer = optax.adam(1e-4)\n",
    "opt_state = optimizer.init(model)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"./runs\")\n",
    "\n",
    "for i in range(0, 100):\n",
    "    x = jnp.ones((10, 80, 100))\n",
    "    model, opt_state, total_loss, reconstruct_loss, commit_loss, codebook_updates, y = (\n",
    "        make_step(model, optimizer, opt_state, x)\n",
    "    )\n",
    "    print(f\"Total loss: {total_loss}\")\n",
    "\n",
    "    # Log codebook updates to TensorBoard\n",
    "    writer.add_scalar(\"Loss/Total\", total_loss, i)\n",
    "    writer.add_scalar(\"Loss/Reconstruct\", reconstruct_loss, i)\n",
    "    writer.add_scalar(\"Loss/Commit\", commit_loss, i)\n",
    "    # writer.add_histogram('Codebook Updates/Cluster Size', jnp.mean(codebook_updates[0], axis=0), i)\n",
    "    # writer.add_scalar('Codebook Updates/Codebook Avg', jnp.mean(codebook_updates[1], axis=0), i)\n",
    "    writer.add_histogram(\n",
    "        \"Codebook Updates/Code ids used\", jnp.sum(codebook_updates[1], axis=(0)), i\n",
    "    )\n",
    "    writer.add_histogram(\n",
    "        \"Codebook Updates/Code means\", jnp.mean(codebook_updates[0][2], axis=(0, 2)), i\n",
    "    )\n",
    "    writer.add_histogram(\n",
    "        \"Codebook Updates/Code stds\", jnp.std(codebook_updates[0][2], axis=(0, 2)), i\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Let's download some data and make runs through to see if it can learn from it ! \n",
    "\n",
    "XTTS has this function that seems to transform the incoming wav files into nice mel_spectrograms. To optimize the time spent loading the data, we'll transform all the data into input arrays first, and then during the run load from those instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "def dvae_wav_to_mel(\n",
    "    wav,\n",
    "    mel_norms_file=\"../experiments/clips_mel_norms.pth\",\n",
    "    mel_norms=None,\n",
    "    device=torch.device(\"cpu\"),\n",
    "):\n",
    "    mel_stft = torchaudio.transforms.MelSpectrogram(\n",
    "        n_fft=1024,\n",
    "        hop_length=256,\n",
    "        win_length=1024,\n",
    "        power=2,\n",
    "        normalized=False,\n",
    "        sample_rate=22050,\n",
    "        f_min=0,\n",
    "        f_max=8000,\n",
    "        n_mels=80,\n",
    "        norm=\"slaney\",\n",
    "    ).to(device)\n",
    "    wav = wav.to(device)\n",
    "    mel = mel_stft(wav)\n",
    "    mel = torch.log(torch.clamp(mel, min=1e-5))\n",
    "    if mel_norms is None:\n",
    "        mel_norms = torch.load(mel_norms_file, map_location=device)\n",
    "    mel = mel / mel_norms.unsqueeze(0).unsqueeze(-1)\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll be using LJSpeech [@ljspeech17] as a dataset because it has just the right amount of samples to get some real training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvjf ./LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy\n",
    "\n",
    "sample_rate = 22050\n",
    "\n",
    "\n",
    "def dvae_wav_to_mel(\n",
    "    wav, mel_norms_file=\"./mel_stats.pth\", mel_norms=None, device=torch.device(\"cpu\")\n",
    "):\n",
    "    mel_stft = torchaudio.transforms.MelSpectrogram(\n",
    "        n_fft=1024,\n",
    "        hop_length=256,\n",
    "        win_length=1024,\n",
    "        power=2,\n",
    "        normalized=False,\n",
    "        sample_rate=22050,\n",
    "        f_min=0,\n",
    "        f_max=8000,\n",
    "        n_mels=80,\n",
    "        norm=\"slaney\",\n",
    "    ).to(device)\n",
    "    wav = wav.to(device)\n",
    "    mel = mel_stft(wav)\n",
    "    mel = torch.log(torch.clamp(mel, min=1e-5))\n",
    "    if mel_norms is None:\n",
    "        mel_norms = torch.load(mel_norms_file, map_location=device)\n",
    "    mel = mel / mel_norms.unsqueeze(0).unsqueeze(-1)\n",
    "    return mel\n",
    "\n",
    "\n",
    "dataset_dest_path = \"dataset/mels\"\n",
    "\n",
    "files = list(Path(\"./LJSpeech-1.1\").rglob(\"*.wav\"))\n",
    "\n",
    "\n",
    "def boop():\n",
    "    for file in tqdm(files):\n",
    "        y, sr = librosa.load(file)\n",
    "        if sr != sample_rate:\n",
    "            y = librosa.resample(y, orig_sr=sr, target_sr=sample_rate)\n",
    "        y = torch.from_numpy(y)\n",
    "        mel = dvae_wav_to_mel(y)\n",
    "\n",
    "        data = mel.cpu().numpy()\n",
    "        save_path = Path(dataset_dest_path, f\"{file.stem}.npy\")\n",
    "        numpy.save(save_path, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a folder full of mel spectrograms ready to be fed into our model, we can start the training ! Below, we do multiple things:\n",
    "\n",
    "Initialize the model, the optimizer that will nugde it based on the losses our function returns, logging with tensorboard and saving the model every epoch.\n",
    "\n",
    "## Actually training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from VQVAE import VQVAE\n",
    "import datetime\n",
    "import os\n",
    "import jax\n",
    "import optax\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "dataset_path = \"dataset/mels\"\n",
    "\n",
    "mel_file_paths = os.listdir(dataset_path)\n",
    "\n",
    "def truncate_and_pad(audio: jax.Array, width: int = 300):\n",
    "    audio_length = audio.shape[1]\n",
    "    \n",
    "    target_length = int(width)\n",
    "    if audio_length > target_length:\n",
    "        audio = audio[:, :target_length]\n",
    "    else:\n",
    "        audio = jnp.pad(audio, ((0, 0), (0, max(0, target_length - audio_length))))\n",
    "    return audio\n",
    "\n",
    "def get_batch(idx: list):\n",
    "    batch = []\n",
    "    for id in idx:\n",
    "        array = jax.numpy.load(os.path.join(dataset_path, mel_file_paths[id]))[0]\n",
    "        padded = truncate_and_pad(array)\n",
    "        batch.append(padded)\n",
    "    return jax.numpy.array(batch)\n",
    "        \n",
    "key = jax.random.PRNGKey(69)\n",
    "\n",
    "key1, key2 = jax.random.split(key, 2)\n",
    "\n",
    "model = VQVAE(key=key1)\n",
    "\n",
    "optimizer = optax.adam(1e-4)\n",
    "opt_state = optimizer.init(model)\n",
    "\n",
    "writer = SummaryWriter(log_dir='./runs/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "fig.show()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    key, grab = jax.random.split(key, 2)\n",
    "    perm = jax.random.permutation(grab, len(mel_file_paths))\n",
    "    eqx.tree_serialise_leaves(f\"checkpoints/{epoch}.eqx\", model)\n",
    "\n",
    "    for i in range(0, len(mel_file_paths), batch_size):\n",
    "        key, grab = jax.random.split(key, 2)\n",
    "        batch_ids = perm[i: i + batch_size]\n",
    "        batch = get_batch(batch_ids)\n",
    "\n",
    "        model, opt_state, total_loss, reconstruct_loss, commit_loss, codebook_updates, y = make_step(model, optimizer, opt_state, batch, grab)\n",
    "\n",
    "        step = epoch * len(mel_file_paths) + i\n",
    "        # Log codebook updates to TensorBoard\n",
    "        writer.add_scalar('Loss/Total', total_loss, step)\n",
    "        writer.add_scalar('Loss/Reconstruct', reconstruct_loss, step)\n",
    "        writer.add_scalar('Loss/Commit', commit_loss, step)\n",
    "        \n",
    "        writer.add_histogram('Codebook Updates/Code ids used', jnp.reshape(codebook_updates[1], -1), step)\n",
    "        writer.add_histogram('Codebook Updates/Code means', jnp.mean(codebook_updates[0][2], axis=(0,2)), step)\n",
    "        writer.add_histogram('Codebook Updates/Code stds', jnp.std(codebook_updates[0][2], axis=(0,2)), step)\n",
    "        if (i // batch_size) % 20 == 0:\n",
    "            print(batch.shape)\n",
    "            print(y.shape)\n",
    "            ax1.clear()\n",
    "            ax2.clear()\n",
    "            ax1.imshow(batch[0], aspect='auto', origin='lower')\n",
    "            ax2.imshow(y[0], aspect='auto', origin='lower')\n",
    "            display(fig)\n",
    "            clear_output(wait=True)\n",
    "    # plt.imshow(y[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little bit of patience, we can see the output image start to resemble more and more the input one ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Result after just 10 minutes of training on a NVIDIA L40](assets/result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tensorboard we can also see that the codewords are progressively all used by the same amount during training. It's beautiful 🥰\n",
    "\n",
    "![Codebooks slowly all being used uniformily.](<assets/uniform.png>)\n",
    "\n",
    "This concludes this chapter, if you have any questions or remarks feel free to reach out to me ! @sxyBoi on Telegram 😅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "nocite: |\n",
    "  @*\n",
    "---\n",
    "\n",
    "# References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
