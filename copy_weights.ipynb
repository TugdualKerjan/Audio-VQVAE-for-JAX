{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to copy the weights for the DVAE from the XTTS model and check that our model spits out the same thing as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download TTS, and the model checkpoint for XTTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\n",
    "!git clone git@github.com:nguyenhoanganh2002/XTTSv2-Finetuning-for-New-Languages.git\n",
    "!mv XTTSv2-Finetuning-for-New-Languages/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now be able to load the model as it is used in the XTTSv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.tts.layers.xtts.dvae import DiscreteVAE\n",
    "import jax\n",
    "import torch\n",
    "import equinox as eqx\n",
    "\n",
    "# Exported from .ipynb using python3 export.py (Exports all cells with the export tag)\n",
    "from VQVAE import VQVAE\n",
    "\n",
    "dvae_pretrained = \"./dvae.pth\"\n",
    "dvae = DiscreteVAE(\n",
    "            channels=80,\n",
    "            normalization=None,\n",
    "            positional_dims=1,\n",
    "            num_tokens=1024,\n",
    "            codebook_dim=512,\n",
    "            hidden_dim=512,\n",
    "            num_resnet_blocks=3,\n",
    "            kernel_size=3,\n",
    "            num_layers=2,\n",
    "            use_transposed_convs=False,\n",
    "        )\n",
    "\n",
    "# Take all the params for the torch model. Print them and map them to our model.\n",
    "        \n",
    "torch_params = {name: param.detach().numpy() for name, param in dvae.named_parameters()}\n",
    "\n",
    "#Grab the checky ones that don't have a name but that we need\n",
    "torch_params[\"dvae.codebook.embed\"] = dvae.codebook.embed\n",
    "dvae.load_state_dict(torch.load(dvae_pretrained), strict=False)\n",
    "print(list(dvae.decoder.state_dict()))\n",
    "\n",
    "#There's probably a better way of doing this lol\n",
    "torch_to_jax_keys = [\n",
    "    (\"encoder.conv1.weight\", \"encoder.0.0.weight\"),\n",
    "    (\"encoder.conv2.weight\", \"encoder.1.0.weight\"),\n",
    "    (\"encoder.res1.conv1.weight\", \"encoder.2.net.0.weight\"),\n",
    "    (\"encoder.res1.conv2.weight\", \"encoder.2.net.2.weight\"),\n",
    "    (\"encoder.res1.conv3.weight\", \"encoder.2.net.4.weight\"),\n",
    "    (\"encoder.res2.conv1.weight\", \"encoder.3.net.0.weight\"),\n",
    "    (\"encoder.res2.conv2.weight\", \"encoder.3.net.2.weight\"),\n",
    "    (\"encoder.res2.conv3.weight\", \"encoder.3.net.4.weight\"),\n",
    "    (\"encoder.res3.conv1.weight\", \"encoder.4.net.0.weight\"),\n",
    "    (\"encoder.res3.conv2.weight\", \"encoder.4.net.2.weight\"),\n",
    "    (\"encoder.res3.conv3.weight\", \"encoder.4.net.4.weight\"),\n",
    "    (\"encoder.conv3.weight\", \"encoder.5.weight\"),\n",
    "    (\"encoder.conv1.bias\", \"encoder.0.0.bias\"),\n",
    "    (\"encoder.conv2.bias\", \"encoder.1.0.bias\"),\n",
    "    (\"encoder.res1.conv1.bias\", \"encoder.2.net.0.bias\"),\n",
    "    (\"encoder.res1.conv2.bias\", \"encoder.2.net.2.bias\"),\n",
    "    (\"encoder.res1.conv3.bias\", \"encoder.2.net.4.bias\"),\n",
    "    (\"encoder.res2.conv1.bias\", \"encoder.3.net.0.bias\"),\n",
    "    (\"encoder.res2.conv2.bias\", \"encoder.3.net.2.bias\"),\n",
    "    (\"encoder.res2.conv3.bias\", \"encoder.3.net.4.bias\"),\n",
    "    (\"encoder.res3.conv1.bias\", \"encoder.4.net.0.bias\"),\n",
    "    (\"encoder.res3.conv2.bias\", \"encoder.4.net.2.bias\"),\n",
    "    (\"encoder.res3.conv3.bias\", \"encoder.4.net.4.bias\"),\n",
    "    (\"encoder.conv3.bias\", \"encoder.5.bias\"),\n",
    "    \n",
    "    (\"quantizer.codebook_avg\", \"dvae.codebook.embed\"),\n",
    "    (\"quantizer.codebook\", \"dvae.codebook.embed\"),\n",
    "\n",
    "    (\"decoder.conv1.weight\", \"decoder.0.weight\"),\n",
    "    (\"decoder.res1.conv1.weight\", \"decoder.1.net.0.weight\"),\n",
    "    (\"decoder.res1.conv2.weight\", \"decoder.1.net.2.weight\"),\n",
    "    (\"decoder.res1.conv3.weight\", \"decoder.1.net.4.weight\"),\n",
    "    (\"decoder.res2.conv1.weight\", \"decoder.2.net.0.weight\"),\n",
    "    (\"decoder.res2.conv2.weight\", \"decoder.2.net.2.weight\"),\n",
    "    (\"decoder.res2.conv3.weight\", \"decoder.2.net.4.weight\"),\n",
    "    (\"decoder.res3.conv1.weight\", \"decoder.3.net.0.weight\"),\n",
    "    (\"decoder.res3.conv2.weight\", \"decoder.3.net.2.weight\"),\n",
    "    (\"decoder.res3.conv3.weight\", \"decoder.3.net.4.weight\"),\n",
    "    (\"decoder.conv2.conv.weight\", \"decoder.4.0.conv.weight\"),\n",
    "    (\"decoder.conv3.conv.weight\", \"decoder.5.0.conv.weight\"),\n",
    "    (\"decoder.conv4.weight\", \"decoder.6.weight\"),\n",
    "    (\"decoder.conv1.bias\", \"decoder.0.bias\"),\n",
    "    (\"decoder.res1.conv1.bias\", \"decoder.1.net.0.bias\"),\n",
    "    (\"decoder.res1.conv2.bias\", \"decoder.1.net.2.bias\"),\n",
    "    (\"decoder.res1.conv3.bias\", \"decoder.1.net.4.bias\"),\n",
    "    (\"decoder.res2.conv1.bias\", \"decoder.2.net.0.bias\"),\n",
    "    (\"decoder.res2.conv2.bias\", \"decoder.2.net.2.bias\"),\n",
    "    (\"decoder.res2.conv3.bias\", \"decoder.2.net.4.bias\"),\n",
    "    (\"decoder.res3.conv1.bias\", \"decoder.3.net.0.bias\"),\n",
    "    (\"decoder.res3.conv2.bias\", \"decoder.3.net.2.bias\"),\n",
    "    (\"decoder.res3.conv3.bias\", \"decoder.3.net.4.bias\"),\n",
    "    (\"decoder.conv2.conv.bias\", \"decoder.4.0.conv.bias\"),\n",
    "    (\"decoder.conv3.conv.bias\", \"decoder.5.0.conv.bias\"),\n",
    "    (\"decoder.conv4.bias\", \"decoder.6.bias\"),\n",
    "]\n",
    "\n",
    "# Initialize the JAX model parameters\n",
    "key = jax.random.PRNGKey(1)\n",
    "model = VQVAE(key=key)\n",
    "\n",
    "# Function to update the JAX model parameters\n",
    "def update_params(path, x):\n",
    "    path_str = '.'.join([p.name for p in path])\n",
    "    for jax_key,torch_key in torch_to_jax_keys:\n",
    "        # print(path_str + \" \" + torch_key)\n",
    "        if jax_key == path_str:\n",
    "            print(path_str)\n",
    "            if \"bias\" in jax_key:\n",
    "                return jax.numpy.expand_dims(torch_params[torch_key], -1)\n",
    "            return jax.numpy.array(torch_params[torch_key])\n",
    "    return x\n",
    "\n",
    "# Update the JAX model parameters\n",
    "model = jax.tree_util.tree_map_with_path(update_params, model)\n",
    "\n",
    "# Replace the encoder in the model with the updated encoder\n",
    "# model = eqx.tree_at(lambda m: m, model, model)\n",
    "print(model.quantizer.codebook.shape)\n",
    "eqx.tree_serialise_leaves(\"./xttsvqvae.eqx\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are chaotic tests I made to check things worked, progressively going through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = torch.ones((1, 80, 300))\n",
    "x1 = jax.numpy.array(x.numpy())  # Convert PyTorch tensor to numpy array before converting to JAX array\n",
    "torch.testing.assert_close(x, torch.from_numpy(np.array(x1)))\n",
    "\n",
    "print(dvae.encoder[1][0])\n",
    "print(model.encoder.conv1)\n",
    "y = dvae.encoder[0](x)  # Apply the encoder to the input tensor\n",
    "y = dvae.encoder[1][0](y)  # Apply the encoder to the input tensor\n",
    "y1= jax.vmap(model.encoder.conv1)(x1)\n",
    "y1= jax.vmap(jax.nn.relu)(y1)\n",
    "y1= jax.vmap(model.encoder.conv2)(y1)\n",
    "# y1 = jax.vmap(jax.nn.relu)(y1)\n",
    "\n",
    "# Convert JAX array back to numpy array and then to PyTorch tensor for comparison\n",
    "torch.testing.assert_close(y, torch.from_numpy(np.array(y1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "x = torch.ones((1, 80, 300))\n",
    "x1 = jax.numpy.array(x.numpy(), dtype=jnp.float64)[\n",
    "    0\n",
    "]  # Convert PyTorch tensor to numpy array before converting to JAX array\n",
    "# torch.testing.assert_close(x, torch.from_numpy(np.array(x1)))\n",
    "\n",
    "input = dvae.encoder(x).permute((0, 2, 1))  # Apply the encoder to the input tensor\n",
    "print(input.shape)\n",
    "flatten = input.reshape(-1, 512)\n",
    "print(flatten.shape)\n",
    "a_squared = flatten.pow(2).sum(1, keepdim=True)\n",
    "print(a_squared.shape)\n",
    "print(f\"Their codebook shape{dvae.codebook.embed.shape}\")\n",
    "\n",
    "b_squared = dvae.codebook.embed.pow(2).sum(0, keepdim=True)\n",
    "print(b_squared.shape)\n",
    "dist = a_squared - 2 * flatten @ dvae.codebook.embed + b_squared\n",
    "soft_codes = -dist\n",
    "_, embed_ind = soft_codes.max(1)\n",
    "embed_onehot = torch.nn.functional.one_hot(embed_ind, dvae.codebook.n_embed).type(\n",
    "    flatten.dtype\n",
    ")\n",
    "embed_ind = embed_ind.view(*input.shape[:-1])\n",
    "y = dvae.codebook.embed_code(embed_ind)\n",
    "print(y.shape)\n",
    "y = y.permute((0, 2, 1))\n",
    "print(y.shape)\n",
    "print(f\"My codebook shape{model.quantizer.codebook.shape}\")\n",
    "y1 = model.encoder(x1)\n",
    "y1 = jax.numpy.permute_dims(y1, (1, 0))\n",
    "print(y1.shape)\n",
    "flatten = jax.numpy.reshape(y1, (-1, model.quantizer.D))\n",
    "a_squared = jnp.sum(jnp.pow(flatten, 2), axis=-1, keepdims=True)\n",
    "print(a_squared.shape)\n",
    "b_squared = jnp.sum(jnp.pow(model.quantizer.codebook, 2), axis=0, keepdims=True)\n",
    "print(b_squared.shape)\n",
    "distance = a_squared + b_squared - 2 * jnp.matmul(flatten, model.quantizer.codebook)\n",
    "\n",
    "\n",
    "codebook_indices = jnp.argmin(distance, axis=-1)\n",
    "torch.testing.assert_close(\n",
    "    embed_ind[0], torch.from_numpy(np.array(codebook_indices)).to(torch.int64)\n",
    ")\n",
    "# codebook_onehot = jax.nn.one_hot(codebook_indices, self.K)\n",
    "z_q = model.quantizer.codebook.T[codebook_indices]\n",
    "# Straight-through estimator\n",
    "print(z_q.shape)\n",
    "z_q = flatten + jax.lax.stop_gradient(z_q - flatten)\n",
    "y1 = jax.numpy.permute_dims(z_q, (1, 0))\n",
    "\n",
    "print(y.shape)\n",
    "# y1, _ = jax.vmap(model.quantizer)(y1)\n",
    "# y = dvae.encoder[1](y)  # Apply the encoder to the input tensor\n",
    "# y1 = jax.vmap(jax.nn.relu)(y1)\n",
    "# y1 = jax.vmap(model.encoder.conv2)(y1)\n",
    "# y1 = jax.vmap(jax.nn.relu)(y1)\n",
    "\n",
    "# y1 = jax.vmap(jax.nn.relu)(y1)\n",
    "\n",
    "# Convert JAX array back to numpy array and then to PyTorch tensor for comparison\n",
    "torch.testing.assert_close(y[0], torch.from_numpy(np.array(y1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "x = torch.ones((1, 80, 300))\n",
    "x1 = jax.numpy.array(x.numpy(), dtype=jnp.float64)[\n",
    "    0\n",
    "]  # Convert PyTorch tensor to numpy array before converting to JAX array\n",
    "# torch.testing.assert_close(x, torch.from_numpy(np.array(x1)))\n",
    "\n",
    "input = dvae.encoder(x).permute((0, 2, 1))  # Apply the encoder to the input tensor\n",
    "y = dvae.codebook(input)[0]\n",
    "y = y.permute((0, 2, 1))\n",
    "y = dvae.decoder(y)\n",
    "y1 = model.encoder(x1)\n",
    "y1, _ = model.quantizer(y1)\n",
    "y1 = model.decoder(y1)\n",
    "print(y.shape)\n",
    "# y1, _ = jax.vmap(model.quantizer)(y1)\n",
    "# y = dvae.encoder[1](y)  # Apply the encoder to the input tensor\n",
    "# y1 = jax.vmap(jax.nn.relu)(y1)\n",
    "# y1 = jax.vmap(model.encoder.conv2)(y1)\n",
    "# y1 = jax.vmap(jax.nn.relu)(y1)\n",
    "\n",
    "# y1 = jax.vmap(jax.nn.relu)(y1)\n",
    "\n",
    "# Convert JAX array back to numpy array and then to PyTorch tensor for comparison\n",
    "torch.testing.assert_close(y[0], torch.from_numpy(np.array(y1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
